# 软件测试代码作业

近年来，大语言模型（LLMs）如 OpenAI Codex、ChatGPT 等在代码生成任务中表现出了强大的潜力。然而，经典的评测指标（如 Pass@k、CodeBLEU）无法充分反映真实世界程序员对代码生成的需求，例如代码质量、可读性、可维护性以及在复杂场景下的实际适用性。

本项目旨在通过模拟真实世界的编程场景，设计和实施一套基于实际需求的代码生成模型测试方法，为LLM的优化和改进提供更实际的指导。

评估代码生成模型在真实世界编程任务中的表现

具体目标

设计真实世界场景的代码生成测试任务。
确定对程序员而言关键的评测维度（如性能、编译通过率等）。


设计涵盖以下类型的代码生成任务：

- 简单算法设计: 解决实际问题的算法实现，如图像处理、路径规划等。
- 针对框架的业务逻辑实现: 生成复杂业务逻辑代码，如订单管理、支付系统等。
- 复杂的业务需求: 生成能与现有系统（如API、数据库等）无缝集成的代码。
- 测试用例生成能力: 根据已有项目代码生成详细完备的测试用例。

由于计算设备资源的限制，模型选择为Llama 3.2